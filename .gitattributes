import torch
import torch.nn as nn
import torch.nn.parameter as Parameter
from layers import GraphAttention, InnerProductDecoder
import numpy as np
import torch_geometric
from torch_geometric.nn import GATConv


class VVAGE(nn.Module):
    def __init__(self, num_nodes, num_features, output_dim, dropout):
        super(VVAGE, self).__init__()
        self.input_dim = num_features
        self.output_dim = output_dim
        self.dropout = dropout
        self.n_samples = num_nodes

        self.encoder = GATConv(self.input_dim, self.output_dim, 1, dropout=dropout)
        self.log_enc = GATConv(self.input_dim, self.output_dim, 1, dropout=dropout)
        self.decoder = InnerProductDecoder(dropout=dropout, act=lambda x: x)

    def forward(self, edge_index, features):
        self.hidden = self.encoder(features, edge_index)
        self.z_log_std = self.log_enc(features, edge_index)
        self.z_mean = self.hidden
        # print("----", self.z_mean.shape, self.z_log_std.shape, self.n_samples, self.output_dim)
        self.z = self.z_mean + torch.randn(self.n_samples, self.output_dim) * torch.exp(self.z_log_std)

        self.reconstructions = self.decoder(self.z)
        return self.z, self.z_mean, self.z_log_std, self.reconstructions
    
    def embed(self, inputs, features):
        self.hidden = self.encoder(inputs, features)
        self.z_log_std = self.log_enc(inputs, features)
        self.z_mean = self.hidden
        self.z = self.z_mean + torch.normal(0.0, std=1, size=[self.n_samples, self.output_dim]) * torch.exp(self.z_log_std)
        return self.z.detach()

class Discriminator(nn.Module):
    def __init__(self, disc_dim, hidden_dim, output_dim):
        super(Discriminator, self).__init__()
        # self.dc_den1 = Parameter(torch.FloatTensor(output_dim, hidden_dim))
        self.dc_den1 = torch.nn.Linear(output_dim, hidden_dim)
        self.act = torch.relu
        # self.dc_den2 = Parameter(torch.FloatTensor(hidden_dim, disc_dim))
        self.dc_den2 = torch.nn.Linear(hidden_dim, disc_dim)
        # self.dc_den3 = Parameter(torch.FloatTensor(disc_dim, 1))
        self.dc_den3 = torch.nn.Linear(disc_dim, 1)
    
    def forward(self, inputs, reuse=False):
        # 没动reuse
        dc1 = self.act(self.dc_den1(inputs))
        dc2= self.act(self.dc_den2(dc1))
        output = self.dc_den3(dc2)
        return output
